{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_as8YpKHGx4e"
      },
      "source": [
        "# Deep Learning\n",
        "## Practical Deep Learning Tutorial with PyTorch - Tutorial NÂ° 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w30hp45dGx4p"
      },
      "source": [
        "### 2020-2021"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cvh8elViGx4q"
      },
      "source": [
        "# Importing necessary libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vezfpqbXGx4r"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import grad\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_blobs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-6XsfvtGx4u"
      },
      "source": [
        "# Adaline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhJXxE9eGx4u"
      },
      "source": [
        "1. Built ADALINE model using the nn.Module class\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "cbvOalUDGx4u"
      },
      "outputs": [],
      "source": [
        "class ADALINE(nn.Module):\n",
        "    def __init__(self, num_features):\n",
        "        super(ADALINE, self).__init__()\n",
        "        self.linear = torch.nn.Linear(num_features, 1)\n",
        "        self.linear.weight.detach().zero_()\n",
        "        self.linear.bias.detach().zero_()\n",
        "\n",
        "    def forward(self, x):\n",
        "        netinputs = self.linear(x)\n",
        "        activations = netinputs\n",
        "        return activations.view(-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXbSSOc4Gx4v"
      },
      "source": [
        "2. Using 'iris.txt', create a binary datasets in 2-D : The last 100 instances of iris described only by the 2nd and 3rd features\n",
        "    \n",
        "    Split the dataset into traing and test sets (70%,30%) \n",
        "\n",
        "    Normalize the dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd \n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "THCwBPnoejW8"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "0SYE1wAhGx4v",
        "outputId": "b9f8df48-37d4-4b56-ad96-001b275c111a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   col0  col1  col2  col3  target\n",
              "0   4.9   3.0   1.4   0.2       1\n",
              "1   4.7   3.2   1.3   0.2       1\n",
              "2   4.6   3.1   1.5   0.2       1\n",
              "3   5.0   3.6   1.4   0.2       1\n",
              "4   5.4   3.9   1.7   0.4       1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d6c8999e-96a1-4d92-9b35-78563a11f835\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>col0</th>\n",
              "      <th>col1</th>\n",
              "      <th>col2</th>\n",
              "      <th>col3</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4.9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4.7</td>\n",
              "      <td>3.2</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4.6</td>\n",
              "      <td>3.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5.0</td>\n",
              "      <td>3.6</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.4</td>\n",
              "      <td>3.9</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.4</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d6c8999e-96a1-4d92-9b35-78563a11f835')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d6c8999e-96a1-4d92-9b35-78563a11f835 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d6c8999e-96a1-4d92-9b35-78563a11f835');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "df = pd.read_csv('iris.txt')\n",
        "df.columns = ['col{}'.format(i) for i in range(df.shape[1])]\n",
        "df=df.rename(columns={df.columns[-1]:'target'})\n",
        "df['target'] = df['target'].apply(lambda x: 0 if x == 'Iris-versicolor' else 1)\n",
        "\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.iloc[:,:-1].values\n",
        "y = df.iloc[:,-1].values\n",
        "X = (X-np.mean(X,axis=0)) / np.std(X,axis=0)\n",
        "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "#converting to torch object \n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
        "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
        "\n",
        "print(X_train.shape,X_train.shape,y_train.shape,y_test.shape)"
      ],
      "metadata": {
        "id": "LX8bcIj9f1IH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e300bed-c004-406b-f3c2-3cf404d05651"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([104, 4]) torch.Size([104, 4]) torch.Size([104]) torch.Size([45])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vjn6Lo3cGx4w"
      },
      "source": [
        "3. Train the model : we will use MSELoss (mean squared error (squared L2 norm)) as loss function. The optimizer is SGD (Stochastic Gradient Descent) with learning rate 0.01."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "WNQpw3o7Gx4w"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def loss_func(yhat, y):\n",
        "    return torch.mean((yhat - y)**2)\n",
        "\n",
        "def train(model, x, y, num_epochs):\n",
        "    cost = []\n",
        "    \n",
        "    for e in range(num_epochs):\n",
        "    \n",
        "        yhat = model.forward(x)\n",
        "        loss = F.mse_loss(yhat, y)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            yhat = model.forward(x)\n",
        "            curr_loss = loss_func(yhat, y)\n",
        "            if (e+1) % 10 == 0:\n",
        "                print(f'Epoch [{e+1}/{num_epochs}], Loss: {curr_loss:.3f}')\n",
        "            cost.append(curr_loss)\n",
        "\n",
        "    return cost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDm5w8JRGx4w"
      },
      "source": [
        "4. Compute the model accuracy "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = ADALINE(num_features=X_train.size(1))\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "cost = train(model, \n",
        "             X_train, y_train.float(),\n",
        "             num_epochs=200)"
      ],
      "metadata": {
        "id": "T0IExpLD9_Ux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ones = torch.ones(y_train.size())\n",
        "zeros = torch.zeros(y_train.size())\n",
        "train_pred = model.forward(X_train)\n",
        "train_acc = torch.mean(\n",
        "    (torch.where(train_pred > 0.5, \n",
        "                 ones, \n",
        "                 zeros).int() == y_train).float())\n",
        "\n",
        "ones = torch.ones(y_test.size())\n",
        "zeros = torch.zeros(y_test.size())\n",
        "test_pred = model.forward(X_test)\n",
        "test_acc = torch.mean(\n",
        "    (torch.where(test_pred > 0.5, \n",
        "                 ones, \n",
        "                 zeros).int() == y_test).float())\n",
        "\n",
        "print(f'Training Accuracy: {train_acc*100: .2f}%')\n",
        "print(f'Test Accuracy: {test_acc*100: .2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "prk4FWfbFtM_",
        "outputId": "11cd0072-066a-47ae-fc57-4ebae96b75dd"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Accuracy:  71.15%\n",
            "Test Accuracy:  75.56%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VauvIZbkGx4x"
      },
      "source": [
        "# Perceptron"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1QZoBzsGx4y"
      },
      "source": [
        "5. Built a Perceptron model using nn.Module class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "qPqPp9svGx4y"
      },
      "outputs": [],
      "source": [
        "class Perceptron(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, num_features):\n",
        "        super(Perceptron, self).__init__()\n",
        "        self.linear = torch.nn.Linear(num_features, 1)\n",
        "        self.linear.weight.detach().zero_()\n",
        "        self.linear.bias.detach().zero_()\n",
        "\n",
        "        \n",
        "    def forward(self, x):\n",
        "        logits = self.linear(x)\n",
        "        probas = torch.sigmoid(logits)\n",
        "        return probas\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKFf-JHKGx4y"
      },
      "source": [
        "6. Load the 'perceptron_toydata' dataset\n",
        "\n",
        "    Split the dataset into train and test sets\n",
        "    \n",
        "    Normalize the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "IeEb5j2RGx4y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39ee4661-f43d-4190-98d9-f2a022f8f20f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([70, 2]) torch.Size([30, 2]) torch.Size([70, 1]) torch.Size([30])\n"
          ]
        }
      ],
      "source": [
        "df=pd.read_csv('perceptron_toydata.txt',sep='\\s+', skipinitialspace=True,names=[f'col{i}' for i in range(3)])\n",
        "\n",
        "X=df.iloc[:,:-1].values \n",
        "y=df['col2'].values\n",
        "X_train,X_test,y_train,y_test = train_test_split( X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "m,s = np.mean(X_train,axis=0), np.std(X_train,axis=0)\n",
        "X_train = (X_train-m)/s\n",
        "X_test = (X_test-m)/s\n",
        "\n",
        "X_train = torch.tensor(X_train,dtype=torch.float32)\n",
        "X_test = torch.tensor(X_test,dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train,dtype=torch.float32).view(-1,1)\n",
        "y_test = torch.tensor(y_test,dtype=torch.float32)\n",
        "print(X_train.shape,X_test.shape,y_train.shape,y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MS3ok31IGx4z"
      },
      "source": [
        "7. Train the perceptron"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "TIyIwFCnGx4z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "384bbab2-9e1c-41a1-a9b3-eaae4cd953e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/100], Accuracy: 0.971 Loss: 0.149\n",
            "Epoch [20/100], Accuracy: 0.971 Loss: 0.100\n",
            "Epoch [30/100], Accuracy: 0.971 Loss: 0.081\n",
            "Epoch [40/100], Accuracy: 0.971 Loss: 0.070\n",
            "Epoch [50/100], Accuracy: 0.971 Loss: 0.063\n",
            "Epoch [60/100], Accuracy: 0.971 Loss: 0.058\n",
            "Epoch [70/100], Accuracy: 0.971 Loss: 0.054\n",
            "Epoch [80/100], Accuracy: 0.986 Loss: 0.051\n",
            "Epoch [90/100], Accuracy: 0.986 Loss: 0.049\n",
            "Epoch [100/100], Accuracy: 0.986 Loss: 0.047\n"
          ]
        }
      ],
      "source": [
        "model = Perceptron(num_features=X_train.size(1))\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "def comp_accuracy(label_var, pred_probas):\n",
        "    pred_labels = torch.where((pred_probas > 0.5), \n",
        "                              torch.tensor([1]), \n",
        "                              torch.tensor([0])).view(-1)\n",
        "    acc = torch.sum(pred_labels == label_var.view(-1)).float() / label_var.size(0)\n",
        "    return acc\n",
        "num_epochs = 100\n",
        "\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    \n",
        "    #### Compute outputs ####\n",
        "    out = model.forward(X_train)\n",
        "    \n",
        "    cost = F.binary_cross_entropy(out, y_train, reduction='sum')\n",
        "    optimizer.zero_grad()\n",
        "    cost.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    with torch.no_grad():     \n",
        "        pred_probas = model.forward(X_train)\n",
        "        acc = comp_accuracy(y_train, pred_probas)\n",
        "        if (epoch+1) % 10 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Accuracy: {acc:.3f} Loss: {F.binary_cross_entropy(pred_probas, y_train):.3f}')\n",
        "\n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vb225wW7Gx40"
      },
      "source": [
        "8. evaluate the model (accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ubtnb6igGx41"
      },
      "outputs": [],
      "source": [
        "### the answer is in the cell above"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0nRdNZpGx41"
      },
      "source": [
        "# Multi Layer Perceptron"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTRo7p9PGx42"
      },
      "source": [
        "Unlike the single-layer perceptron, the Multi Layer Perceptron models have hidden layers\n",
        "between the input and the output layers. After every hidden layer, an activation function \n",
        "is applied to introduce non-linearity. \n",
        "\n",
        "9. Built a simple Multi Layer Perceptron model withe one hidden layer. \n",
        "After the hidden layer, we will use ReLU as activation before the information is sent to the output layer.\n",
        "As an output activation function, we will use Sigmoid. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "K-E8_vffGx42"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(MLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.sigmoid(x)\n",
        "        return x\n",
        "\n",
        "    def train(self, X_train, y_train, learning_rate, num_epochs):\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = torch.optim.SGD(self.parameters(), lr=learning_rate)\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            running_loss = 0.0\n",
        "            for i, data in enumerate(zip(X_train, y_train)):\n",
        "                inputs, labels = data\n",
        "                optimizer.zero_grad()\n",
        "                outputs = self(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                running_loss += loss.item()\n",
        "            if((epoch+1)%10 == 0):\n",
        "                print('Epoch {}/{}, Loss: {:.4f}'.format(epoch+1, num_epochs, running_loss/len(X_train)))\n",
        "\n",
        "    def accuracy(self, X_train, y_train, X_test, y_test):\n",
        "        train_outputs = self(X_train)\n",
        "        _, train_predicted = torch.max(train_outputs.data, 1)\n",
        "        train_correct = (train_predicted == y_train).sum().item()\n",
        "        train_accuracy = train_correct / len(X_train)\n",
        "\n",
        "        test_outputs = self(X_test)\n",
        "        _, test_predicted = torch.max(test_outputs.data, 1)\n",
        "        test_correct = (test_predicted == y_test).sum().item()\n",
        "        test_accuracy = test_correct / len(X_test)\n",
        "\n",
        "        print('Train Accuracy: {:.2f}%'.format(train_accuracy * 100))\n",
        "        print('Test Accuracy: {:.2f}%'.format(test_accuracy * 100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JOWoFhGGx42"
      },
      "source": [
        "10. Create a random datasets and assign binary labels {0,1}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Vn9GA1FgGx42"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Generate random training data\n",
        "np.random.seed(0)\n",
        "input_size = 2\n",
        "train_size = 100\n",
        "test_size = int(train_size*0.2) #20% of the train size\n",
        "X_train = torch.tensor(np.random.rand(train_size, 2), dtype=torch.float32)\n",
        "y_train = torch.tensor(np.random.randint(0, 2, size=(train_size,)), dtype=torch.long)\n",
        "\n",
        "# Generate random test data\n",
        "X_test = torch.tensor(np.random.rand(test_size, input_size), dtype=torch.float32)\n",
        "y_test = torch.tensor(np.random.randint(0, 2, size=(test_size,)), dtype=torch.long)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape,X_test.shape,y_train.shape,y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HeBdlWV541c2",
        "outputId": "60bf884f-6a56-4aa8-a5ab-208957eae961"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([100, 2]) torch.Size([20, 2]) torch.Size([100]) torch.Size([20])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2R92d_SMGx43"
      },
      "source": [
        "11. Define the model with input dimension 2 and hidden dimension 10. \n",
        "Since the task is to classify binary labels, we can use BCELoss (Binary Cross Entropy Loss) as loss function.\n",
        "The optimizer is SGD (Stochastic Gradient Descent) with learning rate 0.01."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "wXL2Ixs_Gx43"
      },
      "outputs": [],
      "source": [
        "hidden_size = 10\n",
        "output_size = 2\n",
        "\n",
        "mlp = MLP(input_size, hidden_size, output_size)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFCQb0kqGx43"
      },
      "source": [
        "12. Check the test loss before the model training and compare it with the test loss after the training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "hDBYcZEfGx43",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b561eb9b-6486-4a16-e439-939f22db2f48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/200, Loss: 0.6836\n",
            "Epoch 20/200, Loss: 0.6833\n",
            "Epoch 30/200, Loss: 0.6830\n",
            "Epoch 40/200, Loss: 0.6827\n",
            "Epoch 50/200, Loss: 0.6823\n",
            "Epoch 60/200, Loss: 0.6819\n",
            "Epoch 70/200, Loss: 0.6817\n",
            "Epoch 80/200, Loss: 0.6814\n",
            "Epoch 90/200, Loss: 0.6811\n",
            "Epoch 100/200, Loss: 0.6808\n",
            "Epoch 110/200, Loss: 0.6807\n",
            "Epoch 120/200, Loss: 0.6802\n",
            "Epoch 130/200, Loss: 0.6801\n",
            "Epoch 140/200, Loss: 0.6799\n",
            "Epoch 150/200, Loss: 0.6797\n",
            "Epoch 160/200, Loss: 0.6795\n",
            "Epoch 170/200, Loss: 0.6794\n",
            "Epoch 180/200, Loss: 0.6792\n",
            "Epoch 190/200, Loss: 0.6790\n",
            "Epoch 200/200, Loss: 0.6790\n",
            "Train Accuracy: 60.00%\n",
            "Test Accuracy: 45.00%\n"
          ]
        }
      ],
      "source": [
        "mlp.train(X_train, y_train, learning_rate=0.1, num_epochs=200)\n",
        "mlp.accuracy(X_train,y_train,X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pk7J7k4wGx43"
      },
      "source": [
        "13. In order to improve the model, you can try out different parameter values for your\n",
        "hyperparameters(ie. hidden dimension size, epoch size, learning rates). You can also \n",
        "try changing the structure of your model (ie. adding more hidden layers) to see if your\n",
        "mode improves. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "P4NSRsjrGx44",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c5e6803-269a-424a-f0c2-b69c4e7200a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "**hiden_size=2 --- epochs=20**\n",
            "\n",
            "Epoch 10/20, Loss: 0.6946\n",
            "Epoch 20/20, Loss: 0.6939\n",
            "Train Accuracy: 58.00%\n",
            "Test Accuracy: 40.00%\n",
            "\n",
            "**hiden_size=2 --- epochs=50**\n",
            "\n",
            "Epoch 10/50, Loss: 0.6921\n",
            "Epoch 20/50, Loss: 0.6895\n",
            "Epoch 30/50, Loss: 0.6876\n",
            "Epoch 40/50, Loss: 0.6864\n",
            "Epoch 50/50, Loss: 0.6854\n",
            "Train Accuracy: 58.00%\n",
            "Test Accuracy: 45.00%\n",
            "\n",
            "**hiden_size=2 --- epochs=200**\n",
            "\n",
            "Epoch 10/200, Loss: 0.6926\n",
            "Epoch 20/200, Loss: 0.6898\n",
            "Epoch 30/200, Loss: 0.6870\n",
            "Epoch 40/200, Loss: 0.6860\n",
            "Epoch 50/200, Loss: 0.6855\n",
            "Epoch 60/200, Loss: 0.6853\n",
            "Epoch 70/200, Loss: 0.6853\n",
            "Epoch 80/200, Loss: 0.6854\n",
            "Epoch 90/200, Loss: 0.6850\n",
            "Epoch 100/200, Loss: 0.6851\n",
            "Epoch 110/200, Loss: 0.6849\n",
            "Epoch 120/200, Loss: 0.6847\n",
            "Epoch 130/200, Loss: 0.6845\n",
            "Epoch 140/200, Loss: 0.6839\n",
            "Epoch 150/200, Loss: 0.6840\n",
            "Epoch 160/200, Loss: 0.6837\n",
            "Epoch 170/200, Loss: 0.6833\n",
            "Epoch 180/200, Loss: 0.6830\n",
            "Epoch 190/200, Loss: 0.6828\n",
            "Epoch 200/200, Loss: 0.6825\n",
            "Train Accuracy: 58.00%\n",
            "Test Accuracy: 45.00%\n",
            "\n",
            "**hiden_size=3 --- epochs=20**\n",
            "\n",
            "Epoch 10/20, Loss: 0.6935\n",
            "Epoch 20/20, Loss: 0.6905\n",
            "Train Accuracy: 56.00%\n",
            "Test Accuracy: 45.00%\n",
            "\n",
            "**hiden_size=3 --- epochs=50**\n",
            "\n",
            "Epoch 10/50, Loss: 0.6955\n",
            "Epoch 20/50, Loss: 0.6937\n",
            "Epoch 30/50, Loss: 0.6898\n",
            "Epoch 40/50, Loss: 0.6875\n",
            "Epoch 50/50, Loss: 0.6865\n",
            "Train Accuracy: 57.00%\n",
            "Test Accuracy: 50.00%\n",
            "\n",
            "**hiden_size=3 --- epochs=200**\n",
            "\n",
            "Epoch 10/200, Loss: 0.6949\n",
            "Epoch 20/200, Loss: 0.6934\n",
            "Epoch 30/200, Loss: 0.6911\n",
            "Epoch 40/200, Loss: 0.6890\n",
            "Epoch 50/200, Loss: 0.6869\n",
            "Epoch 60/200, Loss: 0.6860\n",
            "Epoch 70/200, Loss: 0.6856\n",
            "Epoch 80/200, Loss: 0.6852\n",
            "Epoch 90/200, Loss: 0.6855\n",
            "Epoch 100/200, Loss: 0.6852\n",
            "Epoch 110/200, Loss: 0.6849\n",
            "Epoch 120/200, Loss: 0.6844\n",
            "Epoch 130/200, Loss: 0.6838\n",
            "Epoch 140/200, Loss: 0.6838\n",
            "Epoch 150/200, Loss: 0.6836\n",
            "Epoch 160/200, Loss: 0.6834\n",
            "Epoch 170/200, Loss: 0.6834\n",
            "Epoch 180/200, Loss: 0.6833\n",
            "Epoch 190/200, Loss: 0.6833\n",
            "Epoch 200/200, Loss: 0.6833\n",
            "Train Accuracy: 57.00%\n",
            "Test Accuracy: 55.00%\n",
            "\n",
            "**hiden_size=5 --- epochs=20**\n",
            "\n",
            "Epoch 10/20, Loss: 0.6923\n",
            "Epoch 20/20, Loss: 0.6891\n",
            "Train Accuracy: 57.00%\n",
            "Test Accuracy: 50.00%\n",
            "\n",
            "**hiden_size=5 --- epochs=50**\n",
            "\n",
            "Epoch 10/50, Loss: 0.6907\n",
            "Epoch 20/50, Loss: 0.6875\n",
            "Epoch 30/50, Loss: 0.6865\n",
            "Epoch 40/50, Loss: 0.6861\n",
            "Epoch 50/50, Loss: 0.6859\n",
            "Train Accuracy: 59.00%\n",
            "Test Accuracy: 45.00%\n",
            "\n",
            "**hiden_size=5 --- epochs=200**\n",
            "\n",
            "Epoch 10/200, Loss: 0.6948\n",
            "Epoch 20/200, Loss: 0.6944\n",
            "Epoch 30/200, Loss: 0.6939\n",
            "Epoch 40/200, Loss: 0.6923\n",
            "Epoch 50/200, Loss: 0.6894\n",
            "Epoch 60/200, Loss: 0.6871\n",
            "Epoch 70/200, Loss: 0.6858\n",
            "Epoch 80/200, Loss: 0.6851\n",
            "Epoch 90/200, Loss: 0.6846\n",
            "Epoch 100/200, Loss: 0.6843\n",
            "Epoch 110/200, Loss: 0.6839\n",
            "Epoch 120/200, Loss: 0.6836\n",
            "Epoch 130/200, Loss: 0.6833\n",
            "Epoch 140/200, Loss: 0.6830\n",
            "Epoch 150/200, Loss: 0.6828\n",
            "Epoch 160/200, Loss: 0.6826\n",
            "Epoch 170/200, Loss: 0.6826\n",
            "Epoch 180/200, Loss: 0.6823\n",
            "Epoch 190/200, Loss: 0.6815\n",
            "Epoch 200/200, Loss: 0.6806\n",
            "Train Accuracy: 59.00%\n",
            "Test Accuracy: 45.00%\n"
          ]
        }
      ],
      "source": [
        "hidden_sizes = [2,3,5]\n",
        "epochs = [20,50,200]\n",
        "\n",
        "for h in hidden_sizes:\n",
        "    for e in epochs:\n",
        "        print(f\"\\n**hiden_size={h} --- epochs={e}**\\n\")\n",
        "        mlp = MLP(input_size, h, output_size)\n",
        "        mlp.train(X_train, y_train, learning_rate=0.1, num_epochs=e)\n",
        "        mlp.accuracy(X_train,y_train,X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Same code but with learning rate = 0.01 instead of 0.1"
      ],
      "metadata": {
        "id": "bLw-tMqoACb2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "Yif9RkoTGx44",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2df4465d-2d39-404d-fe98-2f8e315753ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "**hiden_size=2 --- epochs=20**\n",
            "\n",
            "Epoch 10/20, Loss: 0.6937\n",
            "Epoch 20/20, Loss: 0.6921\n",
            "Train Accuracy: 52.00%\n",
            "Test Accuracy: 55.00%\n",
            "\n",
            "**hiden_size=2 --- epochs=50**\n",
            "\n",
            "Epoch 10/50, Loss: 0.6934\n",
            "Epoch 20/50, Loss: 0.6932\n",
            "Epoch 30/50, Loss: 0.6930\n",
            "Epoch 40/50, Loss: 0.6928\n",
            "Epoch 50/50, Loss: 0.6927\n",
            "Train Accuracy: 53.00%\n",
            "Test Accuracy: 60.00%\n",
            "\n",
            "**hiden_size=2 --- epochs=200**\n",
            "\n",
            "Epoch 10/200, Loss: 0.6987\n",
            "Epoch 20/200, Loss: 0.6963\n",
            "Epoch 30/200, Loss: 0.6949\n",
            "Epoch 40/200, Loss: 0.6942\n",
            "Epoch 50/200, Loss: 0.6938\n",
            "Epoch 60/200, Loss: 0.6936\n",
            "Epoch 70/200, Loss: 0.6934\n",
            "Epoch 80/200, Loss: 0.6934\n",
            "Epoch 90/200, Loss: 0.6933\n",
            "Epoch 100/200, Loss: 0.6933\n",
            "Epoch 110/200, Loss: 0.6933\n",
            "Epoch 120/200, Loss: 0.6933\n",
            "Epoch 130/200, Loss: 0.6933\n",
            "Epoch 140/200, Loss: 0.6933\n",
            "Epoch 150/200, Loss: 0.6933\n",
            "Epoch 160/200, Loss: 0.6933\n",
            "Epoch 170/200, Loss: 0.6933\n",
            "Epoch 180/200, Loss: 0.6933\n",
            "Epoch 190/200, Loss: 0.6933\n",
            "Epoch 200/200, Loss: 0.6933\n",
            "Train Accuracy: 50.00%\n",
            "Test Accuracy: 35.00%\n",
            "\n",
            "**hiden_size=3 --- epochs=20**\n",
            "\n",
            "Epoch 10/20, Loss: 0.6944\n",
            "Epoch 20/20, Loss: 0.6936\n",
            "Train Accuracy: 50.00%\n",
            "Test Accuracy: 35.00%\n",
            "\n",
            "**hiden_size=3 --- epochs=50**\n",
            "\n",
            "Epoch 10/50, Loss: 0.6933\n",
            "Epoch 20/50, Loss: 0.6930\n",
            "Epoch 30/50, Loss: 0.6927\n",
            "Epoch 40/50, Loss: 0.6924\n",
            "Epoch 50/50, Loss: 0.6921\n",
            "Train Accuracy: 53.00%\n",
            "Test Accuracy: 50.00%\n",
            "\n",
            "**hiden_size=3 --- epochs=200**\n",
            "\n",
            "Epoch 10/200, Loss: 0.6917\n",
            "Epoch 20/200, Loss: 0.6912\n",
            "Epoch 30/200, Loss: 0.6907\n",
            "Epoch 40/200, Loss: 0.6903\n",
            "Epoch 50/200, Loss: 0.6899\n",
            "Epoch 60/200, Loss: 0.6895\n",
            "Epoch 70/200, Loss: 0.6891\n",
            "Epoch 80/200, Loss: 0.6887\n",
            "Epoch 90/200, Loss: 0.6883\n",
            "Epoch 100/200, Loss: 0.6879\n",
            "Epoch 110/200, Loss: 0.6875\n",
            "Epoch 120/200, Loss: 0.6871\n",
            "Epoch 130/200, Loss: 0.6867\n",
            "Epoch 140/200, Loss: 0.6863\n",
            "Epoch 150/200, Loss: 0.6859\n",
            "Epoch 160/200, Loss: 0.6855\n",
            "Epoch 170/200, Loss: 0.6852\n",
            "Epoch 180/200, Loss: 0.6849\n",
            "Epoch 190/200, Loss: 0.6846\n",
            "Epoch 200/200, Loss: 0.6843\n",
            "Train Accuracy: 60.00%\n",
            "Test Accuracy: 45.00%\n",
            "\n",
            "**hiden_size=5 --- epochs=20**\n",
            "\n",
            "Epoch 10/20, Loss: 0.6935\n",
            "Epoch 20/20, Loss: 0.6934\n",
            "Train Accuracy: 47.00%\n",
            "Test Accuracy: 65.00%\n",
            "\n",
            "**hiden_size=5 --- epochs=50**\n",
            "\n",
            "Epoch 10/50, Loss: 0.6933\n",
            "Epoch 20/50, Loss: 0.6930\n",
            "Epoch 30/50, Loss: 0.6927\n",
            "Epoch 40/50, Loss: 0.6925\n",
            "Epoch 50/50, Loss: 0.6923\n",
            "Train Accuracy: 52.00%\n",
            "Test Accuracy: 35.00%\n",
            "\n",
            "**hiden_size=5 --- epochs=200**\n",
            "\n",
            "Epoch 10/200, Loss: 0.6932\n",
            "Epoch 20/200, Loss: 0.6926\n",
            "Epoch 30/200, Loss: 0.6921\n",
            "Epoch 40/200, Loss: 0.6917\n",
            "Epoch 50/200, Loss: 0.6913\n",
            "Epoch 60/200, Loss: 0.6909\n",
            "Epoch 70/200, Loss: 0.6905\n",
            "Epoch 80/200, Loss: 0.6901\n",
            "Epoch 90/200, Loss: 0.6896\n",
            "Epoch 100/200, Loss: 0.6892\n",
            "Epoch 110/200, Loss: 0.6888\n",
            "Epoch 120/200, Loss: 0.6883\n",
            "Epoch 130/200, Loss: 0.6877\n",
            "Epoch 140/200, Loss: 0.6872\n",
            "Epoch 150/200, Loss: 0.6866\n",
            "Epoch 160/200, Loss: 0.6861\n",
            "Epoch 170/200, Loss: 0.6857\n",
            "Epoch 180/200, Loss: 0.6852\n",
            "Epoch 190/200, Loss: 0.6848\n",
            "Epoch 200/200, Loss: 0.6845\n",
            "Train Accuracy: 60.00%\n",
            "Test Accuracy: 50.00%\n"
          ]
        }
      ],
      "source": [
        "hidden_sizes = [2,3,5]\n",
        "epochs = [20,50,200]\n",
        "\n",
        "for h in hidden_sizes:\n",
        "    for e in epochs:\n",
        "        print(f\"\\n**hiden_size={h} --- epochs={e}**\\n\")\n",
        "        mlp = MLP(input_size, h, output_size)\n",
        "        mlp.train(X_train, y_train, learning_rate=0.01, num_epochs=e)\n",
        "        mlp.accuracy(X_train,y_train,X_test, y_test)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}